# -*- coding: utf-8 -*-
"""pipeline_semantic_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wO4N2IGOAWUIzXcuj-TQAYSVg7THPmEA
"""

import fasttext
from spellchecker import SpellChecker
from nltk.corpus import wordnet as wn
import nltk
import re
import requests
import time
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import sentence_transformers
import nltk
from nltk.stem import WordNetLemmatizer
import enchant
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


nltk.download('wordnet') #synonymes
nltk.download('omw-1.4')
nltk.download('punkt') #lemming

spell = SpellChecker(language='fr')


"""1. spell checking"""

def corriger_requete(query):
    mots = query.split()
    mots_corrigés = [spell.correction(mot) or mot for mot in mots]
    return " ".join(mots_corrigés)


"""3. getting similar words"""

fasttext_model = fasttext.load_model('semantic_search/cc.fr.300.bin')

def get_similar_words(word, k=5):
    try:
        if word not in fasttext_model.words:
            return []
        similar = fasttext_model.get_nearest_neighbors(word, k)
        return [w for _, w in similar]
    except Exception as e:
        print(f"Erreur avec fastText pour le mot '{word}': {e}")
        return []


"""Lemming"""


lemmatizer = WordNetLemmatizer()

def lemming_termes(termes):
    termes_lemmés = set()
    for terme in termes:
        lemma = lemmatizer.lemmatize(terme)
        termes_lemmés.add(lemma)
    return termes_lemmés


dico_fr = enchant.Dict("fr_FR")

def filtrer_mots_francais(termes):
    return {mot for mot in termes if dico_fr.check(mot)}



import requests

# Création d'une session persistante
session = requests.Session()

def get_access_token():
    url = "https://preprod-api.lafoirfouille.fr/occ/v2/token"

    response = session.get(url)

    print("Status:", response.status_code)
    if response.status_code == 200:
        data = response.json()
        return data.get("access_token")
    return None




"""LFF API request"""

def fetch_and_display_products(query):
    access_token = get_access_token()

    url = "https://preprod-api.lafoirfouille.fr/occ/v2/products/search/"
    headers = {
    "Authorization": f"Bearer {access_token}",
    "Accept": "application/json"
    }

    params = {"text": query}
    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        products = response.json()
        if products and 'searchPageData' in products and 'results' in products['searchPageData']:
            product_list = products['searchPageData']['results']
            table_data = [[p.get('name', 'N/A')] for p in product_list]
            if table_data:
              return product_list
    print("erreur ou aucun produit trouvé.")
    return []

"""4. Semantic similarity between products and query"""



def get_similar_products(product_list, query, threshold=0.5):
    product_names = [product.get('name', '') for product in product_list]
    if not product_names:
        return []

    model_sent = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    product_embeddings = model_sent.encode(product_names, convert_to_tensor=True)
    query_embedding = model_sent.encode(query, convert_to_tensor=True).reshape(1, -1)
    product_embeddings = product_embeddings.reshape(len(product_embeddings), -1)

    cosine_scores = cosine_similarity(query_embedding, product_embeddings)[0]

    results= [
        (product_names[i], float(cosine_scores[i]))
        for i in range(len(product_names))
        if cosine_scores[i] > threshold
    ]
    results.sort(key=lambda x: x[1], reverse=True)
    return results



def filtrer_termes(termes_expansion, mots_principaux):
    # garder les termes qui sont plus proches des mots principaux
    termes_filtres = {terme for terme in termes_expansion if any(mot in terme for mot in mots_principaux)}
    return termes_filtres

"""5. Complete pipeline"""

def traiter_requete(query):
    print(f"\nrequête initiale : {query}")
    corrected_query = corriger_requete(query)
    print(f"requête corrigée : {corrected_query}")

    mots = re.findall(r'\w+', corrected_query.lower())
    mots_principaux = set(mots)

    termes_expansion = set(mots)
    
    for mot in mots:

        # voisins fastText
        similaires = get_similar_words(mot)
        similaires_corrigés = {corriger_requete(s).lower() for s in similaires}
        termes_expansion.update(similaires_corrigés)

    print(f"\ntermes d’expansion avant lemming: {termes_expansion}")
    termes_expansion = lemming_termes(termes_expansion)
    print(f"\ntermes d’expansion après lemming: {termes_expansion}")

    print(f"\ntermes d’expansion avant vérification: {termes_expansion}")
    termes_expansion = filtrer_mots_francais(termes_expansion)
    print(f"\ntermes d’expansion après vérification: {termes_expansion}")

    # 4bis. Trier les termes par similarité cosinus avec la requête corrigée
    model_sent = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    query_embedding = model_sent.encode([corrected_query])[0]

    terme_sim_scores = []
    for terme in termes_expansion:
        terme_embedding = model_sent.encode([terme])[0]
        sim_score = cosine_similarity([query_embedding], [terme_embedding])[0][0]
        terme_sim_scores.append((terme, sim_score))

    # Tri décroissant
    termes_tries = [terme for terme, _ in sorted(terme_sim_scores, key=lambda x: x[1], reverse=True)]

    print(f"\n→ Termes triés par similarité cosinus : {termes_tries}")


    produits_par_terme = {}
    seen_ids = set()

    for terme in termes_tries:
        produits = fetch_and_display_products(terme)
        if produits:
            produits_par_terme[terme] = []
            for prod in produits:
                prod_id = prod.get('code')
                if prod_id and prod_id not in seen_ids:
                    seen_ids.add(prod_id)
                    produits_par_terme[terme].append(prod)
        else:
            # si aucun produit n'est trouvé on cherche parmi les voisins fastText avec k=1
            voisins = get_similar_words(terme, k=1)  # limiter à 1 voisin le plus proche
            for voisin in voisins:
                produits_voisin = fetch_and_display_products(voisin)
                if produits_voisin:  # si le voisin donne des produits, on l'utilise
                    print(f"Pas de produits pour le terme \"{terme}\". Utilisation du voisin \"{voisin}\".")
                    produits_par_terme[terme] = produits_voisin
                    break

    if not produits_par_terme:
        print("Aucun produit trouvé.")
        return

    print(f"\nrésultats détaillés par terme :")
    for terme, produits in produits_par_terme.items():
        print(f"\nterme ➜ \"{terme}\"\n" + "-" * 34)
        for prod in produits:
            nom = prod.get("name", "Nom indisponible")
            print(f"  produit : {nom}\n ")
        print("-" * 34)

    tous_les_produits = [p for prods in produits_par_terme.values() for p in prods]
    résultats = get_similar_products(tous_les_produits, corrected_query)

    print("\nproduits similaires à la requête triés dans l'ordre de similarité cosinus:")
    for nom, score in résultats:
        print(f"  {nom} --> {score:.4f}")



traiter_requete("vjissnlle")